# Test plan

### Document Status: Draft
### Document Author: Sebastián Mosca
### Last modified: 12/12/2023

1. Introduction
The project centers around the development of a web portal tailored to provide secure access to employee information. In addition to supporting user authentication, the portal is envisioned to encompass various user management functionalities in the future.

The primary stakeholders vested in the project's success include the development team, the Product Owner overseeing feature development, and members of the Human Resources department within the organization.

For testing purposes, a dedicated Tester, proficient in both manual and automated testing, collaborates with three Developers. The testing process incorporates the delivery of a regression test suite at the conclusion of each three-month development cycle. Moreover, any new functionality introduced must be accompanied by automated tests to meet predefined acceptance criteria.

Specifically, the project will focus on refining the login process and enhancing the visualization of employee data.

Embracing the principles of Continuous Integration (CI) aligned with IT DevOps practices, the project ensures the seamless integration of code changes into the main branch, fostering collaboration and efficiency.

To maintain transparency and traceability, the project emphasizes the creation and upkeep of evidentiary documentation for automated tests, linking test results back to the specified requirements.

Adhering to the SCRUM framework, the project adopts an agile development approach, involving iterative cycles (sprints), regular collaborative meetings, and an ongoing commitment to delivering value incrementally.

In the realm of testing, a key facet is the periodic delivery of a regression test suite, occurring every three months. This strategy helps identify potential issues with existing functionalities, while the inclusion of automated tests with new features ensures adherence to acceptance criteria.

A critical consideration involves risk assessment, particularly in relation to the potential provision of incorrect information to the Human Resources team due to application malfunctions. To mitigate these risks, the team will develop and communicate strategies and contingency plans.

2. Test Strategy
The primary objective of the testing approach is to meticulously validate the accurate implementation of both functional and non-functional requirements. Given the absence of a backend or database, our primary testing focus will be on system testing, involving interactions with the browser.

In our testing strategy, there is a clear emphasis on automation. Automated tests will be an integral part of every new functionality delivery, ensuring efficiency and consistency. For manual testing, we'll adopt an exploratory approach, allowing our skilled QA team to leverage creativity in thoroughly evaluating the system.

Cypress has been chosen as the primary automation framework, providing a robust platform for efficient and reliable automated testing. Concurrently, Jira will serve as the central hub for defining requirements and managing our test cases.

The Tester's role is pivotal throughout the process. They will actively contribute to requirement definition, ensuring that each requirement is testable. During sprint execution, collaboration with Developers will be essential for functional and non-functional testing. Additionally, the Tester will engage in exploratory testing to uncover nuances that may not be evident through scripted scenarios.

Establishing traceability is a key aspect of our strategy. Automated tests and identified issues will be directly linked to the corresponding requirements using tags in Cypress. This meticulous traceability ensures a clear connection between tests, issues, and requirements.

To streamline our testing and issue tracking efforts, a synchronization process will be implemented. This process facilitates the seamless upload of results and test cases from Cypress to Jira. This synchronization ensures visibility and collaboration across the team, enhancing overall efficiency.

By tailoring our testing strategy to the unique characteristics of the application, employing Cypress for automation, and leveraging Jira for requirement definition and test management, we aim to create an efficient, collaborative, and comprehensive testing process that addresses both functional and non-functional dimensions of the system.

3. Resources
In order to successfully conduct the testing activities the following provisions and preconditions described below have to be stablished:

    3.1. Test Equipment 
    The following test equipment will be used during Final Verification.
    
    Testing tools used for Test Automation:
        |Tool           | Version | Author            |
        -----------------------------------------------
        | Cypress       | 13.6.1  | Cypress.io        |
        | Cucumber Open | 9.1.0   | Aslak Hellesøy    |
        | Node.js       | 18.12.1 | OpenJS Foundation |
    Servers Infrastructure:
        N/A - Application Backend not yet available.

    The following devices and operating systems will be used during Final Verification:
        | Client Operating System | Browser                |
        ----------------------------------------------------
        | Windows PC              | Chrome, Microsoft Edge |

    3.2. Roles and responsibilities
    The following key roles are assigned for the testing:
        | Role          | Responsibility                   | Name             |
        -----------------------------------------------------------------------
        | Test manager  | Management of resources, System  | Sebastián Mosca  |
        |               | Test Plan and Report creation    |                  |
        | Test engineer | Manual exploratory testing of    | Sebastián Mosca  |
        |               | System, automated system test    |                  |
        |               | design and implementation and    |                  |
        |               | rest results reporting           |                  |


4. Planning
The system testing activities are separated into two stages.
    
    Preliminary Stage
    The system tests cases are designed and constructed based on analysis of the content.

    During the Preliminary Stage the following activities will be carried on:
    • Automated system test case design and implementation 
    • Test case review and approval 
    • Preliminary test case execution without capturing objective evidence
    • Reporting issues identified
    • Re-testing of implemented issues
    • Exploratory testing
    • Update and execution of the regression tests suite
    

    Final Stage
    Once the system test cases have been executed during the preliminary stage and the, then the final stage of testing will commence on the Release Candidate build.
    
    Summary of final stage activities
    • System test case execution using approved test cases
    • Test case result capturing with objective evidence
    • Re-testing of implemented issues
    • Reporting and recording issues identified

The final test campaign is planned to begin on XX-XXX-2024 and to last until YY-YYY-2024. One round of final testing is planned, however a second final round might happen, if there are urgent issues raised that need to be implemented in the current release.

4.1. Test Activities
This chapter shows the overview of the selected requirements to be tested as well as a detailed test coverage and the list of tests to be executed as well as the responsibilities.

    4.1.1 Overview

    | Test Item    | Scope planned to be fully tested | Scope NOT planned to be tested | Total |
    --------------------------------------------------------------------------------------------
    | Requirements | 2                                | 0                              |       |
    | Issue        | 0                                | 0                              |       |

    4.1.2 Scope planned to be tested

    | Test Item    | UID  | Name                                                     | Product Critically |
    -------------------------------------------------------------------------------------------------------
    | Requirement  | 0003 | IfwecanseethecityoforiginofselectedemployeesinaList.     | High               |
    | Login        | 0001 | Login                                                    | High               |

    | Test Item    | UID  | Name                                                     | Product Critically |
    -------------------------------------------------------------------------------------------------------
    | Requirement  | 0003 | IfwecanseethecityoforiginofselectedemployeesinaList.     | High               |
    | Login        | 0001 | Login                                                    | High               |  

    5.1.3 Planned test execution

    | System Test UID | Req UID     | Name                                    | Status  | Planned Tester  |
    -------------------------------------------------------------------------------------------------------
    | ST001           | 001         | Authentication succeed when using       | Not run | Sebastián Mosca |
    |                 |             | correct credentials                     |         |                 |
    | ST002           | 001         | When wrong credentials are used then    | Not run | Sebastián Mosca |
    |                 |             | authentication fails and user receives  |         |                 |
    |                 |             | visual feedback                         |         |                 |
    | ST003           | 001         | The user is not allowed to insert cer-  | Not run | Sebastián Mosca |
    |                 |             | tain special characters in login input  |         |                 |
    |                 |             | fieldsand sanitized                     |         |                 |
    | ST004           | 001         | After 3 failed loggin attempts the user | Not run | Sebastián Mosca |
    |                 |             | is locked for 10 minutes                |         |                 |
    | ST005           | 001         | Login session expires after 5 minutes   | Not run | Sebastián Mosca |
    |                 |             | without activity                        |         |                 |
    |                 |             | is locked for 10 minutes                |         |                 |
    | ST006           | 003         | View data from a single selected employe| Not run | Sebastián Mosca |
    | ST007           | 003         | Display "No city defined for employee   | Not run | Sebastián Mosca |
    |                 |             | <EmpLN>, <EmpFN>" when viewing data for |         |                 |
    |                 |             | employees with no city defined          |         |                 |
    | ST008           | 003         | View selected data for multiple users   | Not run | Sebastián Mosca |
    |                 |             | at different tree levels                |         |                 |
    | ST009           | 003         | Message "Select at least one employee   | Not run | Sebastián Mosca |
    |                 |             | from the list" is displayed if view     |         |                 |
    |                 |             | selected data clicked while no employee |         |                 |
    |                 |             | is selected                             |         |                 |
    | ST010           | 003         | List is hidden when page loads          |         |                 |    


5. Acceptance Criteria Definition
Test execution will be considered completed when all the tests are runned and its status is either Passed or Failed with their respective evidences attached. In case a test is in Fail status a rationale must be provided describing the issue encountered; If it's an existing/known issue it must be traced in the test execution report, if it's a new issue, a NIC needs to be created and the it has to be as well traced in the test execution report.

6. Reports and Metrics
After the test execution is completed, a test report will be created showing the status for each test executed, its traceability and all the documented evidence gathered during final testing. If there are failed tests or issues have been encountered, a rationale will be added for each item.

7. Defect Management
Whenever an issue appear, a NIC will be created for further analysis by the team. 

8. Risks and Mitigations
N/A - There is no usability requirements.

